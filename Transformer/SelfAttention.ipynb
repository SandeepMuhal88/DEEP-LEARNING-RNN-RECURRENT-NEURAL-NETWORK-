{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a68dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30311492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bf2465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Sandeep muhal Bank money deposit\"\n",
    "\n",
    "def embedding(text,dim):\n",
    "    vocab = set(text.split(\" \"))\n",
    "    vocab_size = len(vocab)\n",
    "    # wordtovec = nn.Embedding(vocab_size,dim)\n",
    "    wordtovec=torch.tensor([[0.1*i for i in range(1,dim+1)] for j in range(vocab_size)])\n",
    "    return wordtovec\n",
    "\n",
    "Wordsembedding=embedding(text,10)\n",
    "\n",
    "# Wordsembedding.weight\n",
    "Wordsembedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80663a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce7f34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Wordsembedding.weight[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Matrix (n x m)\n",
    "# A = torch.tensor([[2,4],\n",
    "#                   [6,7]]) # Shape (2, 3)\n",
    "\n",
    "# # Vector (n)\n",
    "# v_col = torch.tensor([4,5]) # Shape (2)\n",
    "\n",
    "# # Broadcasting the vector to match the shape of the matrix\n",
    "# v_col_broadcasted = v_col.unsqueeze(1)  # Shape (2, 1)\n",
    "# result=A*v_col_broadcasted  # Shape (2, 3)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e100a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Aequence lenth is the size of embedding word vector\n",
    "# Like sandeep is represented as [0.1,0.2,0.3,0.4,0.5] 5 is the sequence length\n",
    "# The dimension of embedding is the size of each word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a44922ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     V_vector\u001b[38;5;241m=\u001b[39mWordsembedding \u001b[38;5;241m*\u001b[39m V\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Q_vector,K_vector,V_vector\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgenerate_Q_K_V_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWordsembedding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mWordsembedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m, in \u001b[0;36mgenerate_Q_K_V_matrices\u001b[1;34m(seq_len, Wordsembedding)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_Q_K_V_matrices\u001b[39m(seq_len,Wordsembedding):\n\u001b[0;32m      3\u001b[0m     Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((seq_len,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m     Q_vector\u001b[38;5;241m=\u001b[39m\u001b[43mWordsembedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\n\u001b[0;32m      5\u001b[0m     K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((seq_len,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m     K_vector\u001b[38;5;241m=\u001b[39mWordsembedding \u001b[38;5;241m*\u001b[39m K\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#Generate Q_sandeep,K_sandeep,V_sandeep matrices\n",
    "def generate_Q_K_V_matrices(seq_len,Wordsembedding):\n",
    "    Q = torch.rand((seq_len,1))\n",
    "    Q_vector=Wordsembedding * Q\n",
    "    K = torch.rand((seq_len,1))\n",
    "    K_vector=Wordsembedding * K\n",
    "    V = torch.rand((seq_len,1))\n",
    "    V_vector=Wordsembedding * V\n",
    "    return Q_vector,K_vector,V_vector\n",
    "\n",
    "\n",
    "generate_Q_K_V_matrices(len(Wordsembedding[1]),Wordsembedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3069a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input embeddings: torch.Size([32, 50, 512])\n",
      "\n",
      "--- Output Shapes (Manual) ---\n",
      "Query (Q) shape: torch.Size([32, 50, 512])\n",
      "Key (K) shape:   torch.Size([32, 50, 512])\n",
      "Value (V) shape: torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Model Parameters ---\n",
    "vocab_size = 1000\n",
    "d_model = 512      # Your embedding dimension\n",
    "seq_length = 50    # Example sequence length\n",
    "batch_size = 32    # Example batch size\n",
    "\n",
    "# --- 2. Setup (Same as before) ---\n",
    "# The lookup table\n",
    "Wordsembedding = nn.Embedding(vocab_size, d_model)\n",
    "# A fake batch of input token IDs\n",
    "input_ids = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length))\n",
    "\n",
    "\n",
    "# --- 3. This is the Function You Want to Build ---\n",
    "\n",
    "def generate_Q_K_V_matrices(input_embeddings, d_model):\n",
    "    \"\"\"\n",
    "    Manually generates Q, K, V from input embeddings.\n",
    "    \"\"\"\n",
    "    # \n",
    "    # Step A: Create the manual weight matrices\n",
    "    # (These are normally inside the nn.Linear layer)\n",
    "    #\n",
    "    W_q = torch.randn(d_model, d_model, requires_grad=True)\n",
    "    W_k = torch.randn(d_model, d_model, requires_grad=True)\n",
    "    W_v = torch.randn(d_model, d_model, requires_grad=True)\n",
    "    \n",
    "    #\n",
    "    # Step B: Perform MATRIX MULTIPLICATION (@), not element-wise (*)\n",
    "    # This projects the embeddings into Q, K, and V spaces\n",
    "    #\n",
    "    # (batch, seq, d_model) @ (d_model, d_model) -> (batch, seq, d_model)\n",
    "    #\n",
    "    Q_vector = input_embeddings @ W_q\n",
    "    K_vector = input_embeddings @ W_k\n",
    "    V_vector = input_embeddings @ W_v\n",
    "    \n",
    "    return Q_vector, K_vector, V_vector\n",
    "\n",
    "# --- 4. How to Use Your Function ---\n",
    "\n",
    "# Step 1: Get the embeddings for your specific input\n",
    "# Shape: (32, 50, 512)\n",
    "embeds = Wordsembedding(input_ids)\n",
    "print(f\"Shape of input embeddings: {embeds.shape}\")\n",
    "\n",
    "# Step 2: Pass those embeddings to your manual function\n",
    "Q_sandeep, K_sandeep, V_sandeep = generate_Q_K_V_matrices(embeds, d_model)\n",
    "\n",
    "print(\"\\n--- Output Shapes (Manual) ---\")\n",
    "print(f\"Query (Q) shape: {Q_sandeep.shape}\")\n",
    "print(f\"Key (K) shape:   {K_sandeep.shape}\")\n",
    "print(f\"Value (V) shape: {V_sandeep.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa1365e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(query, key, value):\n",
    "    d_k=query.size(-1)\n",
    "    scores=torch.matmul(query,key.transpose(-2,-1))/torch.sqrt(torch.tensor(d_k,dtype=torch.float32))\n",
    "    attn_weights=F.softmax(scores,dim=-1)\n",
    "    output=torch.matmul(attn_weights,value)\n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "# self_attention(query_vector, key_vector, value_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".machine (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

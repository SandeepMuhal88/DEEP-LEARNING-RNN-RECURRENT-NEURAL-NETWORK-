{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1a3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project-to-learn\\.machine\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Model training complete.\n",
      "Applying t-SNE for dimensionality reduction...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# --- 5. Visualize Embeddings with t-SNE ---\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# t-SNE is a non-linear dimensionality reduction technique.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# It's excellent for visualizing high-dimensional data in 2D or 3D.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying t-SNE for dimensionality reduction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m tsne \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# We'll apply t-SNE to all vectors except the padding/OOV token at index 0.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m tsne_vectors \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(weights[\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# --- 1. Custom Dataset Creation ---\n",
    "# Let's create a simple dataset of sentences.\n",
    "# We'll include sentences from different topics to see how the embeddings\n",
    "# might group related words together (e.g., sports, finance, weather).\n",
    "dataset = [\n",
    "    \"The stock market saw a significant rally today.\",\n",
    "    \"Investors are optimistic about the new trade deal.\",\n",
    "    \"Heavy rain is expected to cause flooding in the city.\",\n",
    "    \"The football team won the championship in the final game.\",\n",
    "    \"A new smartphone was launched with advanced features.\",\n",
    "    \"Economic growth shows signs of slowing down.\",\n",
    "    \"The star player scored the winning goal.\",\n",
    "    \"A massive storm is approaching the coast.\",\n",
    "    \"Technology stocks are leading the market surge.\",\n",
    "    \"The coach celebrated the victory with his team.\",\n",
    "    \"Heavy snowfall is causing travel disruptions.\",\n",
    "    \"Financial analysts predict a market correction.\",\n",
    "    \"A new software update includes bug fixes.\",\n",
    "    \"The quarterback threw a touchdown pass.\",\n",
    "    \"The weather forecast predicts sunny skies.\",\n",
    "    \"The company's stock price soared.\",\n",
    "]\n",
    "\n",
    "# --- 2. Data Tokenization and Padding ---\n",
    "# Tokenization converts words to integers. We use a Tokenizer from Keras.\n",
    "# We will use a smaller vocabulary size to keep the visualization manageable.\n",
    "max_words = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "# Convert sentences to sequences of integers.\n",
    "sequences = tokenizer.texts_to_sequences(dataset)\n",
    "\n",
    "# Pad the sequences to ensure they all have the same length.\n",
    "# This is a requirement for feeding data into a neural network.\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# We'll use a dummy binary classification task to train the model.\n",
    "# The model doesn't need to be good at this task; the goal is just\n",
    "# to force the embedding layer to learn meaningful representations.\n",
    "labels = np.array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1])\n",
    "\n",
    "# --- 3. Model Training with Embedding Layer ---\n",
    "# Define the parameters for our embedding layer.\n",
    "embedding_dim = 16  # We'll choose a small dimension for simplicity.\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Total number of unique words + 1 for padding/OOV.\n",
    "\n",
    "# Build a simple sequential model with an Embedding layer.\n",
    "model = Sequential()\n",
    "# The Embedding layer takes integer inputs and maps them to dense vectors.\n",
    "# The weights are initialized randomly and trained during the process.\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid')) # A simple classification layer.\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model. The embedding layer's weights are adjusted here.\n",
    "print(\"Training the model...\")\n",
    "model.fit(padded_sequences, labels, epochs=50, verbose=0)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 4. Extract Learned Embedding Vectors ---\n",
    "# The embedding vectors are the weights of the first layer in our model.\n",
    "# We can access them directly.\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# --- 5. Visualize Embeddings with t-SNE ---\n",
    "# t-SNE is a non-linear dimensionality reduction technique.\n",
    "# It's excellent for visualizing high-dimensional data in 2D or 3D.\n",
    "print(\"Applying t-SNE for dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, perplexity=5, n_iter=1000, random_state=42)\n",
    "# We'll apply t-SNE to all vectors except the padding/OOV token at index 0.\n",
    "tsne_vectors = tsne.fit_transform(weights[1:])\n",
    "\n",
    "# Create a dictionary to map integer index back to words for plotting labels.\n",
    "reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(tsne_vectors)):\n",
    "    word_index = i + 1\n",
    "    # Check if the word exists in our reverse index.\n",
    "    if word_index in reverse_word_index:\n",
    "        word = reverse_word_index[word_index]\n",
    "        # Plot the point.\n",
    "        plt.scatter(tsne_vectors[i, 0], tsne_vectors[i, 1])\n",
    "        # Annotate the point with the word.\n",
    "        plt.annotate(word,\n",
    "                     xy=(tsne_vectors[i, 0], tsne_vectors[i, 1]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "\n",
    "plt.title('t-SNE Visualization of Custom Word Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

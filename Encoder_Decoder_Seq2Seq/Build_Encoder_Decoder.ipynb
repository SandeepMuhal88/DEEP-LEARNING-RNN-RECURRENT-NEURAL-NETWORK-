{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# 1. Data Preparation (Synthetic for simplicity)\n",
    "# Reverse a sequence of characters, e.g., \"abc\" -> \"cba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples=10000, max_len=10):\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.randint(1, max_len + 1)\n",
    "        input_seq = ''.join(np.random.choice(list(chars), length))\n",
    "        target_seq = input_seq[::-1]\n",
    "        input_texts.append(input_seq)\n",
    "        target_texts.append(target_seq)\n",
    "    return input_texts, target_texts\n",
    "\n",
    "input_texts, target_texts = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e092653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hcttiypvd',\n",
       " 'hlqjskwyh',\n",
       " 'hxkjvnp',\n",
       " 'ykorsuc',\n",
       " 'hpxcsi',\n",
       " 'a',\n",
       " 'vtbfcze',\n",
       " 'b',\n",
       " 'hdj',\n",
       " 'nxgkshfpa',\n",
       " 'yeit',\n",
       " 'hgcoq',\n",
       " 'f',\n",
       " 'yfd',\n",
       " 'byrsrj',\n",
       " 'nzuxonwg',\n",
       " 'k',\n",
       " 'jmtyphjz',\n",
       " 'dsl',\n",
       " 'rqt',\n",
       " 'uhpcmuri',\n",
       " 'vq',\n",
       " 'ejlmwelbi',\n",
       " 'lucmpwzjm',\n",
       " 'ydgesbhm',\n",
       " 'bqkyiffmn',\n",
       " 'eg',\n",
       " 'sxyvxea',\n",
       " 'qhvmasnyvs',\n",
       " 'sks',\n",
       " 'm',\n",
       " 'oigwpqhtcv',\n",
       " 'jujg',\n",
       " 'tx',\n",
       " 'qs',\n",
       " 'qlmdgjipik',\n",
       " 'obyil',\n",
       " 'lj',\n",
       " 'nqurlup',\n",
       " 'vwaaewa',\n",
       " 'qy',\n",
       " 'k',\n",
       " 'v',\n",
       " 'fszxyg',\n",
       " 'fbh',\n",
       " 'j',\n",
       " 'zaujz',\n",
       " 'cnonf',\n",
       " 'xebmquj',\n",
       " 'jgzcehkqu',\n",
       " 'navdm',\n",
       " 'm',\n",
       " 'zt',\n",
       " 'jc',\n",
       " 'bmhkc',\n",
       " 'kmu',\n",
       " 'byveyzu',\n",
       " 'phqzzuzsl',\n",
       " 'em',\n",
       " 'p',\n",
       " 'sf',\n",
       " 'ifcud',\n",
       " 'bjiamaa',\n",
       " 'ylbrpqgvg',\n",
       " 'sm',\n",
       " 'gxctptyx',\n",
       " 'pq',\n",
       " 'vxlhgcegz',\n",
       " 'kpuje',\n",
       " 'xsalowxi',\n",
       " 'qlzcomudc',\n",
       " 'tkjduirrez',\n",
       " 'ddhvtmwtwa',\n",
       " 'lm',\n",
       " 'gjs',\n",
       " 'odthigbwda',\n",
       " 'ialm',\n",
       " 'lzapi',\n",
       " 'mrmhda',\n",
       " 'g',\n",
       " 'xrwofyqe',\n",
       " 'ajy',\n",
       " 'mvratn',\n",
       " 'thdjnnungl',\n",
       " 'lrvxvuw',\n",
       " 'hqktxsov',\n",
       " 'dvxywdij',\n",
       " 'rclhf',\n",
       " 'lyumg',\n",
       " 'rvxfymq',\n",
       " 'tbeut',\n",
       " 'xgh',\n",
       " 'hzuqxa',\n",
       " 'sx',\n",
       " 'ehpicjxeed',\n",
       " 'f',\n",
       " 'l',\n",
       " 'r',\n",
       " 'aal',\n",
       " 'rfbuluhne',\n",
       " 'rc',\n",
       " 'ltxsghjgz',\n",
       " 'hysnhcbawx',\n",
       " 'kbllrszlk',\n",
       " 'wbdk',\n",
       " 'zdltbp',\n",
       " 'zsigh',\n",
       " 'npx',\n",
       " 'wmpfwtxe',\n",
       " 'ybgawnbpg',\n",
       " 'luyo',\n",
       " 'ktfpgbuzy',\n",
       " 'jvzonsocjp',\n",
       " 'otudichyr',\n",
       " 'mhjnni',\n",
       " 'txkio',\n",
       " 'pxgdwqvw',\n",
       " 'lcvhbax',\n",
       " 'bvvzdx',\n",
       " 'uypm',\n",
       " 'wx',\n",
       " 'qiqad',\n",
       " 'uyrwabakxh',\n",
       " 'lggn',\n",
       " 'ipdne',\n",
       " 'vsgebjdjqr',\n",
       " 'rhqw',\n",
       " 'upp',\n",
       " 'ntxpyq',\n",
       " 'dd',\n",
       " 'ofayuxot',\n",
       " 'wiaji',\n",
       " 'nnzoksvrm',\n",
       " 't',\n",
       " 'vaxuotzclj',\n",
       " 'eojam',\n",
       " 'nfd',\n",
       " 'xpa',\n",
       " 'gbicoefsad',\n",
       " 'ouruayacv',\n",
       " 'elxkt',\n",
       " 'rswxsnplvu',\n",
       " 'bp',\n",
       " 'v',\n",
       " 'or',\n",
       " 'psahki',\n",
       " 'qikgd',\n",
       " 'jmtxsnaee',\n",
       " 'sagob',\n",
       " 'udolptcr',\n",
       " 'b',\n",
       " 's',\n",
       " 'euaafkrr',\n",
       " 'cslhfne',\n",
       " 'knbhcsfey',\n",
       " 'qkzoudnr',\n",
       " 'syhhcrca',\n",
       " 'rbtqvefx',\n",
       " 'uruath',\n",
       " 'xsukfqzu',\n",
       " 'cvhevaf',\n",
       " 'tc',\n",
       " 'qbtadcgc',\n",
       " 'jokwngplu',\n",
       " 'ifb',\n",
       " 'ahdqbc',\n",
       " 'fivrpk',\n",
       " 'xp',\n",
       " 'vmoczivgih',\n",
       " 'hpl',\n",
       " 'qpxctjwm',\n",
       " 'drtfqvd',\n",
       " 'gcgdygydp',\n",
       " 'gquh',\n",
       " 'imkrqgtt',\n",
       " 'rwjcme',\n",
       " 'jqfuooh',\n",
       " 'g',\n",
       " 'a',\n",
       " 'iypwhedfx',\n",
       " 'xaajra',\n",
       " 'fvcrheyt',\n",
       " 'hpg',\n",
       " 'cmqq',\n",
       " 'g',\n",
       " 'qjcbtiqhs',\n",
       " 'ifictlbig',\n",
       " 'tcqtlkjd',\n",
       " 'ymxjmkaaj',\n",
       " 'bayyvf',\n",
       " 'quz',\n",
       " 'oreb',\n",
       " 'k',\n",
       " 'an',\n",
       " 'z',\n",
       " 'erdfa',\n",
       " 'prbqoqzepy',\n",
       " 'cdec',\n",
       " 'layhakk',\n",
       " 'mpsrxpikv',\n",
       " 'zqmtccmbi',\n",
       " 'swoxhpkhvx',\n",
       " 'ymioafcgus',\n",
       " 'o',\n",
       " 'ihyd',\n",
       " 'rnlybajc',\n",
       " 'am',\n",
       " 'ndamjogi',\n",
       " 'tsbpuh',\n",
       " 'm',\n",
       " 'gqbtoyw',\n",
       " 'yb',\n",
       " 'dxocrmqpbk',\n",
       " 'wdmfbzd',\n",
       " 'iz',\n",
       " 'xesxdjhy',\n",
       " 'radm',\n",
       " 'u',\n",
       " 'txqyuuggua',\n",
       " 'hsjyzvmrb',\n",
       " 'zf',\n",
       " 'cxgwug',\n",
       " 'moavyhju',\n",
       " 'nmbgxb',\n",
       " 'ysbhlwr',\n",
       " 'irh',\n",
       " 'tseud',\n",
       " 'pkkfruub',\n",
       " 'c',\n",
       " 'xil',\n",
       " 'axus',\n",
       " 'lbh',\n",
       " 'mnz',\n",
       " 'xctgief',\n",
       " 'qhtf',\n",
       " 'muzfgveotk',\n",
       " 'jdph',\n",
       " 'uxnc',\n",
       " 'zwvilu',\n",
       " 'ictigwwxq',\n",
       " 'iowcrll',\n",
       " 'nfuvwy',\n",
       " 'jdefnqh',\n",
       " 'yph',\n",
       " 'crc',\n",
       " 'zoweszr',\n",
       " 'bcdnkqcut',\n",
       " 'soxi',\n",
       " 'jmfjngdt',\n",
       " 'cpoenvvnr',\n",
       " 'z',\n",
       " 'vp',\n",
       " 'yetyeocqws',\n",
       " 'xtaje',\n",
       " 'xqsezmhoh',\n",
       " 'zs',\n",
       " 'gytzydcvfu',\n",
       " 'wtpa',\n",
       " 'vntkerjzjy',\n",
       " 'n',\n",
       " 'g',\n",
       " 'lvicchd',\n",
       " 'f',\n",
       " 'apfv',\n",
       " 'wcahegsnke',\n",
       " 'dwtbthh',\n",
       " 'zerdmtt',\n",
       " 'tmiuqf',\n",
       " 'pnpxwwt',\n",
       " 'atdxtlg',\n",
       " 'izqbdrix',\n",
       " 'pmbcx',\n",
       " 'jzy',\n",
       " 'wrj',\n",
       " 'gsjskmkbz',\n",
       " 'hvilpi',\n",
       " 'lndtbygz',\n",
       " 'wk',\n",
       " 'ecwqy',\n",
       " 'krrk',\n",
       " 'cpuwbxgl',\n",
       " 'zafnkydpyf',\n",
       " 'xwmeju',\n",
       " 'jczitz',\n",
       " 'dmyuvovnw',\n",
       " 'uhcurtln',\n",
       " 'v',\n",
       " 't',\n",
       " 'ik',\n",
       " 'wxcqxzljut',\n",
       " 'fdk',\n",
       " 'kglnuqm',\n",
       " 'haihsg',\n",
       " 'umnqgdw',\n",
       " 'fjijc',\n",
       " 'tjrhipdi',\n",
       " 'vdmpm',\n",
       " 'gozsxjvu',\n",
       " 'plwkhsm',\n",
       " 'srbmjsbt',\n",
       " 'letqeohjom',\n",
       " 'kn',\n",
       " 'ekhdpv',\n",
       " 'mxdzxffmh',\n",
       " 'tjypjbqx',\n",
       " 'jrgyahstd',\n",
       " 'teo',\n",
       " 'bbzt',\n",
       " 'wkmqqvabe',\n",
       " 'xyfjcsh',\n",
       " 'cjtmir',\n",
       " 'dkgeoimc',\n",
       " 'hing',\n",
       " 'hjxzrx',\n",
       " 'd',\n",
       " 'tmi',\n",
       " 'pntuvie',\n",
       " 'rszy',\n",
       " 'ehyysv',\n",
       " 'na',\n",
       " 'zwjsyw',\n",
       " 'xofjkwxzd',\n",
       " 'ui',\n",
       " 'hffw',\n",
       " 'cjxrigkuvk',\n",
       " 'oinvbxkht',\n",
       " 'cbkeiuji',\n",
       " 'ut',\n",
       " 'auorq',\n",
       " 'ljc',\n",
       " 'kot',\n",
       " 'clalzo',\n",
       " 'mxxwzuyvav',\n",
       " 'hcafla',\n",
       " 'ld',\n",
       " 'i',\n",
       " 'hiwsaiuzl',\n",
       " 'wl',\n",
       " 'lilugp',\n",
       " 'yff',\n",
       " 'jqyzguiyn',\n",
       " 'nfuchj',\n",
       " 'pdmgqhb',\n",
       " 'fdj',\n",
       " 'olexmt',\n",
       " 'gpjva',\n",
       " 'faux',\n",
       " 'dvmglp',\n",
       " 'bmxlapmmh',\n",
       " 'tlr',\n",
       " 'o',\n",
       " 'uhnxois',\n",
       " 'rcojh',\n",
       " 'mbrtujbk',\n",
       " 'rlsgqigdzc',\n",
       " 'qbhjtfxfn',\n",
       " 'lhlr',\n",
       " 'xfs',\n",
       " 'kny',\n",
       " 'e',\n",
       " 'i',\n",
       " 'kcsnqgfx',\n",
       " 'eblql',\n",
       " 'kq',\n",
       " 'l',\n",
       " 'nvocarui',\n",
       " 'srl',\n",
       " 'rnkrucmlka',\n",
       " 'wxgddjef',\n",
       " 'oo',\n",
       " 'otelgmqu',\n",
       " 'jdro',\n",
       " 'ripext',\n",
       " 'snfbkh',\n",
       " 'vv',\n",
       " 'nmpzdaxvu',\n",
       " 'rlwbgicbft',\n",
       " 'fqqquyi',\n",
       " 'bdotsud',\n",
       " 'dpvssgvonw',\n",
       " 'wcq',\n",
       " 'fru',\n",
       " 'dec',\n",
       " 'edvidye',\n",
       " 'ossprfou',\n",
       " 'ofnsc',\n",
       " 'xpewt',\n",
       " 'btgfh',\n",
       " 'nn',\n",
       " 'gboyf',\n",
       " 'dpegzpta',\n",
       " 'xaduvm',\n",
       " 'javtacuw',\n",
       " 'dqtqal',\n",
       " 'a',\n",
       " 'kt',\n",
       " 'nxklngrlfp',\n",
       " 'pqt',\n",
       " 'codndf',\n",
       " 'qjfntguqex',\n",
       " 'cpnj',\n",
       " 'hmitijf',\n",
       " 'ghfg',\n",
       " 'uhfpi',\n",
       " 'hwgsvv',\n",
       " 'jlavax',\n",
       " 'ybhbllex',\n",
       " 'gkrtminlkg',\n",
       " 'jy',\n",
       " 'fs',\n",
       " 'k',\n",
       " 'uofdgiqv',\n",
       " 'vkldbbmjn',\n",
       " 'v',\n",
       " 'nayf',\n",
       " 'rp',\n",
       " 'ff',\n",
       " 'ztvorj',\n",
       " 'mmjbh',\n",
       " 'sjiyugnn',\n",
       " 'fmoil',\n",
       " 'zkoxtu',\n",
       " 'ruei',\n",
       " 'shxy',\n",
       " 'jqqhm',\n",
       " 'whmryjkdyb',\n",
       " 'ftiv',\n",
       " 'v',\n",
       " 'uszwsp',\n",
       " 'x',\n",
       " 'hqxlnysby',\n",
       " 'slvol',\n",
       " 'hvwbrxnscq',\n",
       " 'iscmhgb',\n",
       " 'veeeicxw',\n",
       " 'gcn',\n",
       " 'vbmja',\n",
       " 'ompr',\n",
       " 'xarsudasb',\n",
       " 'gdey',\n",
       " 'wxomn',\n",
       " 'rls',\n",
       " 'tek',\n",
       " 'zuehtjwzbq',\n",
       " 'onexs',\n",
       " 's',\n",
       " 'vmr',\n",
       " 'klgjromt',\n",
       " 'tf',\n",
       " 'zx',\n",
       " 'bhpllg',\n",
       " 'tzejyaa',\n",
       " 'xabqcje',\n",
       " 'ceuywl',\n",
       " 'tpw',\n",
       " 'kyom',\n",
       " 'zxdcry',\n",
       " 'mqaqpql',\n",
       " 'cvl',\n",
       " 'zephyyjn',\n",
       " 'cycopao',\n",
       " 'fhjmfgpdm',\n",
       " 'd',\n",
       " 'tebbbdx',\n",
       " 'r',\n",
       " 'mogzxqe',\n",
       " 'dx',\n",
       " 'izms',\n",
       " 'o',\n",
       " 'a',\n",
       " 'nvgvzafws',\n",
       " 'xzmcnl',\n",
       " 'dihs',\n",
       " 'axralvpqov',\n",
       " 'k',\n",
       " 'ela',\n",
       " 'utgjtxri',\n",
       " 'mkumfx',\n",
       " 'pnmfsgegfr',\n",
       " 'qp',\n",
       " 'hld',\n",
       " 'q',\n",
       " 'pccfkeur',\n",
       " 'zmkdsjijp',\n",
       " 'fkybqfosy',\n",
       " 'nbnb',\n",
       " 'ccenrar',\n",
       " 'dwsl',\n",
       " 'bdfunwzfli',\n",
       " 'jx',\n",
       " 'njegjro',\n",
       " 'bsogjgkhgv',\n",
       " 'ybzz',\n",
       " 'xmbvbcmh',\n",
       " 'ybzzufafa',\n",
       " 'qhmj',\n",
       " 'nmzjbbk',\n",
       " 'qon',\n",
       " 'oviob',\n",
       " 'ydmskzhl',\n",
       " 'ujd',\n",
       " 'fpmzt',\n",
       " 'k',\n",
       " 'dq',\n",
       " 'arabnutlr',\n",
       " 'izg',\n",
       " 'dwt',\n",
       " 'pcutsvy',\n",
       " 'kbnojn',\n",
       " 'sbjiuf',\n",
       " 'ydypgvtwm',\n",
       " 'cjhon',\n",
       " 'lmhpcp',\n",
       " 'zp',\n",
       " 'hnytr',\n",
       " 'tslwdlad',\n",
       " 'dm',\n",
       " 'g',\n",
       " 'mrk',\n",
       " 'mgrzvpuzzr',\n",
       " 'vknmxq',\n",
       " 'bqpgbfhb',\n",
       " 'usbxkkado',\n",
       " 'ixymyva',\n",
       " 'tnnlfk',\n",
       " 'dlbxur',\n",
       " 'lehspebdmj',\n",
       " 'zeq',\n",
       " 'zhbnoqf',\n",
       " 'pad',\n",
       " 'rmdtiisvo',\n",
       " 'hth',\n",
       " 'asouqqntc',\n",
       " 'cfdoow',\n",
       " 'rv',\n",
       " 'voymrflaxf',\n",
       " 'ao',\n",
       " 'amaop',\n",
       " 'ryb',\n",
       " 'miiahml',\n",
       " 'y',\n",
       " 'rztluretlq',\n",
       " 'tfbfm',\n",
       " 'zohkqjhoto',\n",
       " 'adi',\n",
       " 'txnqrcb',\n",
       " 'cba',\n",
       " 'juuxhnk',\n",
       " 'oacijbi',\n",
       " 'kk',\n",
       " 'jj',\n",
       " 'mnskryfat',\n",
       " 'q',\n",
       " 'katrjaegn',\n",
       " 'oakbh',\n",
       " 'hqpadosvqm',\n",
       " 'nqhepr',\n",
       " 'yfbrpxu',\n",
       " 'lg',\n",
       " 'tjnxrfkeeg',\n",
       " 'jvfwv',\n",
       " 'jankoltu',\n",
       " 'tzxluqe',\n",
       " 'e',\n",
       " 'hvhgeoe',\n",
       " 'kvdtubbgf',\n",
       " 'ygg',\n",
       " 'ynz',\n",
       " 'pa',\n",
       " 'hsemqru',\n",
       " 'wasooyogk',\n",
       " 'rr',\n",
       " 'sp',\n",
       " 'xqzusyhitn',\n",
       " 'fjybacenzj',\n",
       " 'fmn',\n",
       " 'xq',\n",
       " 'btcasikbzg',\n",
       " 'id',\n",
       " 'p',\n",
       " 'suzdgip',\n",
       " 'voibai',\n",
       " 'fx',\n",
       " 'fpwbnzpfil',\n",
       " 'jsnpxlyjij',\n",
       " 'sub',\n",
       " 'd',\n",
       " 'ina',\n",
       " 'mmafzjpgs',\n",
       " 'mhbmgrl',\n",
       " 'xdi',\n",
       " 'lm',\n",
       " 'djednvm',\n",
       " 'rbn',\n",
       " 'rlxsauo',\n",
       " 'lfyqqpn',\n",
       " 'pffgrn',\n",
       " 'fxi',\n",
       " 'e',\n",
       " 'yypkpsfh',\n",
       " 'rmytmyi',\n",
       " 'ajvnbitro',\n",
       " 'wscwc',\n",
       " 'bevtpvkow',\n",
       " 'ycaycspujd',\n",
       " 'ktwpplucks',\n",
       " 'ipkn',\n",
       " 'wexhesx',\n",
       " 'vplqyswof',\n",
       " 'mqjdbbtbuj',\n",
       " 'rwaqwxmfa',\n",
       " 'scmwb',\n",
       " 'gjuzy',\n",
       " 'rucvvvbu',\n",
       " 'hf',\n",
       " 'uwaenjhbx',\n",
       " 'i',\n",
       " 'erqe',\n",
       " 'k',\n",
       " 'jtl',\n",
       " 'vzcc',\n",
       " 'y',\n",
       " 'iytoj',\n",
       " 'cqagfz',\n",
       " 'jbknk',\n",
       " 'i',\n",
       " 'mnd',\n",
       " 'bayxzptqvk',\n",
       " 'rvcjx',\n",
       " 'duodbnyyhp',\n",
       " 'kc',\n",
       " 'ebanacacv',\n",
       " 'krkgrp',\n",
       " 'mleufx',\n",
       " 'o',\n",
       " 'wsd',\n",
       " 'vlkiuaun',\n",
       " 'kxhggvrwv',\n",
       " 'feabkxuy',\n",
       " 'boo',\n",
       " 'xhimfkgogf',\n",
       " 'iliwjehpay',\n",
       " 'nibfbtwdi',\n",
       " 's',\n",
       " 'kdrj',\n",
       " 'eck',\n",
       " 'sob',\n",
       " 'qbnmychhn',\n",
       " 'ongl',\n",
       " 'xqqdt',\n",
       " 'rjjbitp',\n",
       " 'brdovfb',\n",
       " 'caxn',\n",
       " 'wleruka',\n",
       " 'z',\n",
       " 'seppymq',\n",
       " 'tzlgvv',\n",
       " 'gj',\n",
       " 'maxiek',\n",
       " 'ay',\n",
       " 'sx',\n",
       " 't',\n",
       " 'wdpa',\n",
       " 'euvrycq',\n",
       " 'duvul',\n",
       " 'vpqz',\n",
       " 'lbwj',\n",
       " 'aud',\n",
       " 'kydrqyac',\n",
       " 'jakmwsjqae',\n",
       " 'yyzkbrdj',\n",
       " 'qfgh',\n",
       " 'jlfwjjp',\n",
       " 'mqtpdgtbf',\n",
       " 'n',\n",
       " 'lujb',\n",
       " 'ftxvrequ',\n",
       " 'ngyic',\n",
       " 'geugtm',\n",
       " 'phjprx',\n",
       " 'pwhez',\n",
       " 'uq',\n",
       " 'galfuwqmm',\n",
       " 'qk',\n",
       " 'a',\n",
       " 'nxasbe',\n",
       " 'u',\n",
       " 'sbrwj',\n",
       " 'f',\n",
       " 'flj',\n",
       " 'xabfrjuahn',\n",
       " 'zddkiggirg',\n",
       " 'ihza',\n",
       " 'ov',\n",
       " 'exfqkofm',\n",
       " 'tu',\n",
       " 'sx',\n",
       " 'xj',\n",
       " 'sdm',\n",
       " 'lohzulsji',\n",
       " 'doxjcldjvj',\n",
       " 'ce',\n",
       " 'ewvgigl',\n",
       " 'hvahmpkgnv',\n",
       " 'y',\n",
       " 'cmhmy',\n",
       " 'jqfowpgz',\n",
       " 'g',\n",
       " 'ciamblsubv',\n",
       " 'wmebygv',\n",
       " 'rwphpw',\n",
       " 'offqhojt',\n",
       " 'tuvrj',\n",
       " 'ksijgukb',\n",
       " 'zzc',\n",
       " 'chbwx',\n",
       " 'uy',\n",
       " 'pbdoe',\n",
       " 'pjxb',\n",
       " 's',\n",
       " 'dyjkgutscc',\n",
       " 's',\n",
       " 'ycard',\n",
       " 'kbwysw',\n",
       " 'g',\n",
       " 'iu',\n",
       " 'kflnmuy',\n",
       " 'tsxsidae',\n",
       " 'wv',\n",
       " 'lgbsfb',\n",
       " 'cxdepzs',\n",
       " 'retymtzdy',\n",
       " 'cnqyojfblp',\n",
       " 'tqd',\n",
       " 'khadha',\n",
       " 'npxpec',\n",
       " 'udgyhmtwt',\n",
       " 'zi',\n",
       " 'd',\n",
       " 'ree',\n",
       " 'hot',\n",
       " 'd',\n",
       " 'je',\n",
       " 'flbubvxg',\n",
       " 'tjbjjvfom',\n",
       " 'zygtwl',\n",
       " 'uwuhcxhb',\n",
       " 'wycvewtre',\n",
       " 'rjhur',\n",
       " 'ptymwzmdhu',\n",
       " 'fap',\n",
       " 'ghgpgwnkgv',\n",
       " 'vvje',\n",
       " 'i',\n",
       " 'lggrr',\n",
       " 'pw',\n",
       " 'dh',\n",
       " 'rhzmkfuu',\n",
       " 'qhpdf',\n",
       " 'daonqpoa',\n",
       " 'btka',\n",
       " 'kfyyw',\n",
       " 'blpkzl',\n",
       " 'qjyup',\n",
       " 'udlvkd',\n",
       " 'p',\n",
       " 'pgtpnhzuqo',\n",
       " 'fnazghald',\n",
       " 'hw',\n",
       " 'aahr',\n",
       " 'pimfoqvntt',\n",
       " 'pasqjjebba',\n",
       " 'uuet',\n",
       " 'mwgtlaen',\n",
       " 'napkqfg',\n",
       " 'r',\n",
       " 'gievi',\n",
       " 'osytg',\n",
       " 'x',\n",
       " 'bdz',\n",
       " 'vzdwdqzjsc',\n",
       " 'pwjnnlxw',\n",
       " 'auykdabqi',\n",
       " 'lhvr',\n",
       " 'fjsfqbxmp',\n",
       " 'bay',\n",
       " 'vklnpzpkcd',\n",
       " 'mpfin',\n",
       " 'fvr',\n",
       " 'bzkcwqdbi',\n",
       " 'lyscm',\n",
       " 'cdckbpnjad',\n",
       " 'qvlgpi',\n",
       " 'bpnhylhzi',\n",
       " 'eml',\n",
       " 'yyjijty',\n",
       " 'ahidxqitww',\n",
       " 'ewqaiomvtu',\n",
       " 'yd',\n",
       " 'g',\n",
       " 'gjmi',\n",
       " 'so',\n",
       " 'aviz',\n",
       " 'jdoyfr',\n",
       " 'yiacus',\n",
       " 'cmyizz',\n",
       " 'isdcdedfw',\n",
       " 'xdtmvdodqr',\n",
       " 'ndfgxsph',\n",
       " 'oyv',\n",
       " 'ocdjci',\n",
       " 'yzlxiuvcj',\n",
       " 'ykxbtg',\n",
       " 'vwuicjhyq',\n",
       " 'r',\n",
       " 'xmkbaibc',\n",
       " 'avaykxcxnl',\n",
       " 'nq',\n",
       " 's',\n",
       " 'j',\n",
       " 'ljlx',\n",
       " 'ztu',\n",
       " 'll',\n",
       " 'wtyfvvtz',\n",
       " 'aspwey',\n",
       " 'vbxhuksw',\n",
       " 'wdysqzs',\n",
       " 'xy',\n",
       " 'bvepjpsu',\n",
       " 'oow',\n",
       " 'k',\n",
       " 'j',\n",
       " 'bgzoq',\n",
       " 'f',\n",
       " 'xrkmt',\n",
       " 'xcdunxb',\n",
       " 'ism',\n",
       " 'zlmttcsz',\n",
       " 'lfw',\n",
       " 'ifbjl',\n",
       " 'wea',\n",
       " 'mnsl',\n",
       " 'ywexdka',\n",
       " 'vhh',\n",
       " 'phroome',\n",
       " 'ellqupivd',\n",
       " 'dmhodr',\n",
       " 'tkmmie',\n",
       " 'hsgcnmyei',\n",
       " 'biwmpgvb',\n",
       " 'osfdsjxsvu',\n",
       " 'luxpoj',\n",
       " 'jejc',\n",
       " 'iizse',\n",
       " 'yzpvqp',\n",
       " 'lsyulqeack',\n",
       " 'evv',\n",
       " 'fzdrg',\n",
       " 'diya',\n",
       " 'a',\n",
       " 'nycvzqpbdo',\n",
       " 'silfo',\n",
       " 'g',\n",
       " 'asy',\n",
       " 'psibsrvxi',\n",
       " 'jnmzmhpyi',\n",
       " 'xzy',\n",
       " 'aovu',\n",
       " 'mft',\n",
       " 'vpmjqcf',\n",
       " 'anwfguf',\n",
       " 'kxcis',\n",
       " 'gkivcf',\n",
       " 'rehqvxnvgq',\n",
       " 'rkvbvah',\n",
       " 'dkqx',\n",
       " 'cj',\n",
       " 'kfbx',\n",
       " 'tdbaxqbvi',\n",
       " 'uuve',\n",
       " 'ro',\n",
       " 'wuip',\n",
       " 'lmdrubgq',\n",
       " 'oeatwtbqs',\n",
       " 'flmtonn',\n",
       " 'ol',\n",
       " 'txsyxaeo',\n",
       " 'lcdlkceja',\n",
       " 'mepkr',\n",
       " 'ydso',\n",
       " 'vqrugh',\n",
       " 'z',\n",
       " 'mxkcya',\n",
       " 'xnwjt',\n",
       " 'onf',\n",
       " 'xo',\n",
       " 'lz',\n",
       " 'avjk',\n",
       " 'xk',\n",
       " 'vxncmwrngh',\n",
       " 'tvsfhqzpt',\n",
       " 'ofliql',\n",
       " 'ffb',\n",
       " 'wbnq',\n",
       " 'nlekj',\n",
       " 'ta',\n",
       " 'jtzh',\n",
       " 'galw',\n",
       " 'fcipvtefb',\n",
       " 'o',\n",
       " 'vlbhtd',\n",
       " 'wairbbssd',\n",
       " 'oiv',\n",
       " 'q',\n",
       " 'p',\n",
       " 'r',\n",
       " 'syhgqzwf',\n",
       " 'go',\n",
       " 'lodge',\n",
       " 'lihnkffsz',\n",
       " 'ij',\n",
       " 'ed',\n",
       " 'ubzamhdzpz',\n",
       " 'vaguyplhn',\n",
       " 'vlyfczs',\n",
       " 'm',\n",
       " 'trpalj',\n",
       " 'roikfskp',\n",
       " 's',\n",
       " 'ej',\n",
       " 'lbizmxari',\n",
       " 'e',\n",
       " 'dhdvt',\n",
       " 'tbfeltih',\n",
       " 'jqk',\n",
       " 'tsqkwmcth',\n",
       " 'kwdqtlq',\n",
       " 'oyziagnq',\n",
       " 'j',\n",
       " 'l',\n",
       " 'ffeaysa',\n",
       " 'yxga',\n",
       " 'kdkfvio',\n",
       " 'c',\n",
       " 'cxismfnjyt',\n",
       " 'my',\n",
       " 'rzqr',\n",
       " 'fejq',\n",
       " 'nnsb',\n",
       " 'se',\n",
       " 'lial',\n",
       " 'yhihzpmqjn',\n",
       " 'gkgxvjduy',\n",
       " 'eqmkgx',\n",
       " 'rsq',\n",
       " 'kxizixvnol',\n",
       " 'e',\n",
       " 'uinhhrvq',\n",
       " 'ghlxzqj',\n",
       " 'aer',\n",
       " 'khtxtwehqy',\n",
       " 'dbvrzwveeh',\n",
       " 'nddawpfl',\n",
       " 'g',\n",
       " 'x',\n",
       " 'ayspku',\n",
       " 'aqbwmsnlab',\n",
       " 'wsnfa',\n",
       " 'opttso',\n",
       " 'mm',\n",
       " 'u',\n",
       " 'z',\n",
       " 'bkdvjexbpt',\n",
       " 'x',\n",
       " 'wguyhapn',\n",
       " 'sqpuytgcxs',\n",
       " 'ywqtroo',\n",
       " 'shrvpcspov',\n",
       " 'f',\n",
       " 'rmszxjhgh',\n",
       " 'lt',\n",
       " 'm',\n",
       " 'zhwwzo',\n",
       " 'rfldce',\n",
       " 'oygkczyd',\n",
       " 'zzgjoqozoo',\n",
       " 'eksscygcm',\n",
       " 'lrhmialcdl',\n",
       " 'fdamvag',\n",
       " 'vz',\n",
       " 'wwdwwt',\n",
       " 'bivylm',\n",
       " 'qxifyqwzq',\n",
       " 'xtry',\n",
       " 'ljqz',\n",
       " 'x',\n",
       " 'ket',\n",
       " 'jjoqwffwd',\n",
       " 'ek',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633caf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and create vocabulary\n",
    "input_vocab = sorted(list(set(\"\".join(input_texts))))\n",
    "target_vocab = sorted(list(set(\"\".join(target_texts))))\n",
    "\n",
    "input_vocab_size = len(input_vocab) + 3  # +3 for <pad>, <start>, and <end>\n",
    "target_vocab_size = len(target_vocab) + 3  # +3 for <pad>, <start>, and <end>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8229f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b3bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i + 2) for i, char in enumerate(input_vocab)])\n",
    "input_token_index[\"<pad>\"] = 0\n",
    "input_token_index[\"<start>\"] = 1\n",
    "input_token_index[\"<end>\"] = len(input_vocab) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8dfff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_index = dict([(char, i + 2) for i, char in enumerate(target_vocab)])\n",
    "target_token_index[\"<pad>\"] = 0\n",
    "target_token_index[\"<start>\"] = 1\n",
    "target_token_index[\"<end>\"] = len(target_vocab) + 2\n",
    "\n",
    "reverse_target_token_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) + 1 # +1 for <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0847e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "def vectorize_sequences(texts, token_index, max_len):\n",
    "    vectorized_data = np.zeros((len(texts), max_len), dtype='int32')\n",
    "    for i, text in enumerate(texts):\n",
    "        for t, char in enumerate(text):\n",
    "            vectorized_data[i, t] = token_index[char]\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15dc8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = vectorize_sequences(input_texts, input_token_index, max_encoder_seq_length)\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length), dtype='int32')\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, target_vocab_size), dtype='float32')\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    decoder_input_data[i, 0] = target_token_index['<start>']\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t + 1] = target_token_index[char]\n",
    "        decoder_target_data[i, t, target_token_index[char]] = 1.\n",
    "    decoder_target_data[i, len(target_text), target_token_index['<end>']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e6ffd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build the Encoder-Decoder with Attention Model\n",
    "\n",
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.gru.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1648927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention (Bahdanau Attention)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # values encoder output shape == (batch_size, max_len, hidden size)\n",
    "\n",
    "        # expand_dims to add time axis to query\n",
    "        query_with_time_axis = tf.expand_dims(query, 1) # (batch_size, 1, hidden size)\n",
    "\n",
    "        # score shape == (batch_size, max_len, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_len, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8a5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_len, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d2f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_dim, units)\n",
    "decoder = Decoder(target_vocab_size, embedding_dim, units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c764eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Step\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([target_token_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e0cfbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_18612\\3002211202.py\", line 16, in train_step  *\n        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n    File \"d:\\Project-to-learn\\.machine\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_18612\\430463626.py\", line 30, in call\n        return x, state, attention_wei\n\n    NameError: Exception encountered when calling Decoder.call().\n    \n    \u001b[1mname 'attention_wei' is not defined\u001b[0m\n    \n    Arguments received by Decoder.call():\n      • x=tf.Tensor(shape=(64, 1), dtype=int32)\n      • hidden=tf.Tensor(shape=(64, 512), dtype=float32)\n      • enc_output=tf.Tensor(shape=(64, 10, 512), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, targ)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;28mlen\u001b[39m(encoder_input_data) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size)):\n\u001b[1;32m---> 13\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Project-to-learn\\.machine\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewrdi21zq.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(inp, targ, enc_hidden)\u001b[0m\n\u001b[0;32m     31\u001b[0m     t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdec_hidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdec_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mint\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(targ)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     35\u001b[0m variables \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(encoder)\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(decoder)\u001b[38;5;241m.\u001b[39mtrainable_variables\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewrdi21zq.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m dec_hidden, dec_input, loss\n\u001b[0;32m     25\u001b[0m t \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m---> 26\u001b[0m (predictions, dec_hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(loss_function, (targ[:, t], predictions), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32md:\\Project-to-learn\\.machine\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36mDecoder.call\u001b[1;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# output shape == (batch_size, vocab)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, state, \u001b[43mattention_wei\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: in user code:\n\n    File \"C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_18612\\3002211202.py\", line 16, in train_step  *\n        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n    File \"d:\\Project-to-learn\\.machine\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_18612\\430463626.py\", line 30, in call\n        return x, state, attention_wei\n\n    NameError: Exception encountered when calling Decoder.call().\n    \n    \u001b[1mname 'attention_wei' is not defined\u001b[0m\n    \n    Arguments received by Decoder.call():\n      • x=tf.Tensor(shape=(64, 1), dtype=int32)\n      • hidden=tf.Tensor(shape=(64, 512), dtype=float32)\n      • enc_output=tf.Tensor(shape=(64, 10, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "\n",
    "# Prepare dataset for training\n",
    "BUFFER_SIZE = len(encoder_input_data)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((encoder_input_data, decoder_input_data)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    enc_hidden = encoder.initialize_hidden_state(batch_size)\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(len(encoder_input_data) // batch_size)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss.numpy() / (len(encoder_input_data) // batch_size):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".machine (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
